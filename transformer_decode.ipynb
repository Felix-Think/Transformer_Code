{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "CQye183iqc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "nDe7Hh3zrs"
      },
      "source": [
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_length, device = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(\n",
        "            num_embeddings = vocab_size,\n",
        "            embedding_dim = embed_dim\n",
        "        )\n",
        "        self.position_embedding = nn.Embedding(\n",
        "            num_embeddings = max_length,\n",
        "            embedding_dim = embed_dim\n",
        "        )\n",
        "    def forward(self, output):\n",
        "        batch_size, seq_length = output.size()\n",
        "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device)\n",
        "        output1 = self.word_embedding(output)\n",
        "        output2 = self.position_embedding(positions)\n",
        "        return output1 + output2 # batch_size, seq_length, embed_dim"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "AHzCxWjtUC"
      },
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            batch_first = True # return shape is batch_size, seq_length, embed_dim\n",
        "        )\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(in_features = embed_dim,\n",
        "                      out_features = ff_dim,\n",
        "                      bias = True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = ff_dim,\n",
        "                      out_features = embed_dim,\n",
        "                      bias = True)\n",
        "        )\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.norm2 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.norm3 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.dropout1 = nn.Dropout(p = dropout)\n",
        "        self.dropout2 = nn.Dropout(p = dropout)\n",
        "        self.dropout3 = nn.Dropout(p = dropout)\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output, _ = self.attn(x, x, x, attn_mask = tgt_mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "\n",
        "        attn_output, _ = self.cross_attn(out1, enc_output, enc_output, attn_mask = src_mask) # out1 is query or is input of decode, enc_output is key and value of encoder\n",
        "        attn_output = self.dropout2(attn_output)\n",
        "        out2 = self.norm2(out1 + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out3 = self.norm3(out2 + ffn_output)\n",
        "        return out3"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "x1yNGL9k4t"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, tgt_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim, dropout= 0.1, device = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.embedding = TokenAndPositionEmbedding(\n",
        "            tgt_vocab_size, embed_dim, max_length, device\n",
        "        )\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerDecoderBlock(\n",
        "                    embed_dim, num_heads, ff_dim, dropout\n",
        "                )for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        output = self.embedding(x)\n",
        "        # Because in this case we don't have encoder, so we must use the same input for query, key, and value\n",
        "        enc_output = self.embedding(enc_output)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, enc_output, src_mask, tgt_mask) # output is query or is input of decode, enc_output is key and value of encoder\n",
        "        return output"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "H00HAdNP7T"
      },
      "source": [
        "tgt_vocab_size = 2000\n",
        "src_vocab_size = 1000\n",
        "max_length = 100\n",
        "batch_size = 128\n",
        "embed_dim = 200\n",
        "ff_dim = 256\n",
        "num_heads = 4\n",
        "num_layers = 2"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "h6d5bBzYv4"
      },
      "source": [
        "tgt_seq_len = 100\n",
        "tgt_mask = torch.ones(tgt_seq_len, tgt_seq_len)\n",
        "tgt_mask "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "AIfA7zmGuJ"
      },
      "source": [
        "#tgt_mask = tgt_mask.triu(diagonal = 1) # diagonal = 1 means we exclude the diagonal and below it\n",
        "# But in this case we trust use triu\n",
        "tgt_mask = torch.triu(tgt_mask)\n",
        "tgt_mask"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [0., 1., 1.,  ..., 1., 1., 1.],\n        [0., 0., 1.,  ..., 1., 1., 1.],\n        ...,\n        [0., 0., 0.,  ..., 1., 1., 1.],\n        [0., 0., 0.,  ..., 0., 1., 1.],\n        [0., 0., 0.,  ..., 0., 0., 1.]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "YJvrMDJrBZ"
      },
      "source": [
        "tgt_mask = tgt_mask.transpose(0, 1) # 0, 1 means we transpose the first and second dimension\n",
        "tgt_mask"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [1., 1., 0.,  ..., 0., 0., 0.],\n        [1., 1., 1.,  ..., 0., 0., 0.],\n        ...,\n        [1., 1., 1.,  ..., 1., 0., 0.],\n        [1., 1., 1.,  ..., 1., 1., 0.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "dIy83DU1UT"
      },
      "source": [
        "tgt_mask = tgt_mask.masked_fill(tgt_mask == 0, float('-inf'))\n",
        "tgt_mask"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "tensor([[1., -inf, -inf,  ..., -inf, -inf, -inf],\n        [1., 1., -inf,  ..., -inf, -inf, -inf],\n        [1., 1., 1.,  ..., -inf, -inf, -inf],\n        ...,\n        [1., 1., 1.,  ..., 1., -inf, -inf],\n        [1., 1., 1.,  ..., 1., 1., -inf],\n        [1., 1., 1.,  ..., 1., 1., 1.]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "3LehVG5EG0"
      },
      "source": [
        "tgt_mask = tgt_mask.masked_fill(tgt_mask == 1, 0)\n",
        "tgt_mask\n",
        "src_mask = tgt_mask"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "uJFEGlQhwT"
      },
      "source": [
        "model = TransformerDecoder(tgt_vocab_size, embed_dim, max_length, num_layers, num_heads, ff_dim)\n",
        "src = torch.randint(\n",
        "    high = 2,\n",
        "    size = (batch_size, max_length),\n",
        "    dtype = torch.int64\n",
        ")\n",
        "\n",
        "tgt = torch.randint(\n",
        "    high = 2,\n",
        "    size = (batch_size, max_length),\n",
        "    dtype = torch.int64\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "8Vz1ewKnyY"
      },
      "source": [
        "prediction = model(src, tgt, src_mask, tgt_mask)\n",
        "prediction.shape # batch_size, seq_length, embed_dim"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "torch.Size([128, 100, 200])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}