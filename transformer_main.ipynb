{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "MGjqXj5zN0"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "oJCYSLNpM1"
      },
      "source": [
        "## Token Anh Position Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "5m9G4tYiJX"
      },
      "source": [
        "class TokenAndPositionEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_length, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(\n",
        "            num_embeddings = vocab_size,\n",
        "            embedding_dim = embed_dim\n",
        "        )\n",
        "        self.position_embedding = nn.Embedding(\n",
        "            num_embeddings = max_length,\n",
        "            embedding_dim = embed_dim\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length = x.size()\n",
        "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device)\n",
        "        output_1 = self.word_embedding(x)\n",
        "        output_2 = self.position_embedding(positions)\n",
        "        return output_1 + output_2"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "HTUPsMp9HC"
      },
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # batch_size = 2, seq_length = 3\n",
        "\n",
        "vocab_size = 100\n",
        "embed_dim = 128\n",
        "max_length = 3\n",
        "device = 'cpu'\n",
        "\n",
        "embedding = TokenAndPositionEmbedding(vocab_size, embed_dim, max_length, device)\n",
        "\n",
        "embedding(x).shape  # torch.Size([2, 3, 128])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "torch.Size([2, 3, 128])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "reCilEbNaf"
      },
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout = 0.1, device = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            batch_first = True\n",
        "        )\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(in_features = embed_dim,\n",
        "                      out_features = ff_dim,\n",
        "                      bias = True\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = ff_dim,\n",
        "                      out_features = embed_dim,\n",
        "                      bias = True)\n",
        "        )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p = dropout)\n",
        "        self.dropout_2 = nn.Dropout(p = dropout)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        attn_output, _ = self.attn(query, key, value) # self-attention\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        output_1 = self.layernorm_1(query + attn_output) # residual connection\n",
        "        ffn_output = self.ffn(output_1)\n",
        "        ffn_output = self.dropout_2(ffn_output)\n",
        "        output_2 = self.layernorm_2(output_1 + ffn_output) # residual connection\n",
        "        return output_2"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "DcxigkF88H"
      },
      "source": [
        "batch_size = 2\n",
        "max_length = 5\n",
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 256\n",
        "dropout = 0.1\n",
        "device = 'cpu'\n",
        "\n",
        "encoder_block = TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout, device)\n",
        "embedding = TokenAndPositionEmbedding(vocab_size, embed_dim, max_length, device)\n",
        "input = torch.randint(\n",
        "    high = 2,\n",
        "    size = (batch_size, max_length),\n",
        "    dtype = torch.int64\n",
        ")\n",
        "\n",
        "embedded_input = embedding(input) # batch_size, max_length, embed_dim\n",
        "output = encoder_block(embedded_input, embedded_input, embedded_input)\n",
        "print(output.shape) # torch.Size([2, 5, 128])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "torch.Size([2, 5, 128])\n"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "xdGyeR6sa5"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, ff_dim, num_layers, max_length, dropout = 0.1, device = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        # Token and Position Embedding\n",
        "        self.embedding = TokenAndPositionEmbedding(vocab_size, embedding_dim, max_length, device)\n",
        "        # Transformer Encoder Blocks\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerEncoderBlock(embedding_dim, num_heads, ff_dim, dropout\n",
        "                )for _ in range(num_layers)\n",
        "            ] \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, output, output)\n",
        "        return output"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "NmxVLHzx3i"
      },
      "source": [
        "batch_size = 128\n",
        "max_length = 5\n",
        "embed_dim = 128\n",
        "num_heads = 8\n",
        "ff_dim = 256\n",
        "dropout = 0.1\n",
        "num_layers = 4\n",
        "device = 'cpu'\n",
        "\n",
        "encoder = TransformerEncoder(vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_length, dropout, device)\n",
        "input = torch.randint(\n",
        "    high = 2,\n",
        "    size = (batch_size, max_length),\n",
        "    dtype = torch.int64\n",
        ")\n",
        "encoder(input).shape # torch.Size([2, 5, 128])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "torch.Size([128, 5, 128])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "ScOUp3K1Ia"
      },
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(in_features = embed_dim,\n",
        "                      out_features = ff_dim,\n",
        "                      bias = True),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features = ff_dim,\n",
        "                      out_features = embed_dim,\n",
        "                      bias= True)\n",
        "        )\n",
        "        self.layernorm_1 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.layernorm_2 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.layernorm_3 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.dropout_1 = nn.Dropout(p = dropout)\n",
        "        self.dropout_2 = nn.Dropout(p = dropout)\n",
        "        self.dropout_3 = nn.Dropout(p = dropout)\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output, _ = self.attn(x, x, x, attn_mask = tgt_mask) # self-attention\n",
        "        attn_output = self.dropout_1(attn_output)\n",
        "        output_1 = self.layernorm_1(x + attn_output) # residual connection\n",
        "        attn_output, _ = self.cross_attn(output_1, enc_output, enc_output, attn_mask = src_mask) # cross-attention\n",
        "        attn_output = self.dropout_2(attn_output)\n",
        "        output_2 = self.layernorm_2(output_1 + attn_output)\n",
        "        ffn_output = self.ffn(output_2)\n",
        "        ffn_output = self.dropout_3(ffn_output)\n",
        "        output_3 = self.layernorm_3(output_2 + ffn_output) # residual connection\n",
        "        return output_3"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "nfcZpEVBSa"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self,tgt_vocab_size,  embed_dim, max_length, num_heads, ff_dim, num_layers, dropout = 0.1, device = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.embedding = TokenAndPositionEmbedding(tgt_vocab_size, embed_dim, max_length, device)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerDecoderBlock(embed_dim, num_heads, ff_dim, dropout\n",
        "                )for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        output = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output, enc_output, src_mask, tgt_mask)\n",
        "        return output"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "OLuIljcOQo"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, tgt_vocab_size, src_vocab_size, tgt_max_length, src_max_length, embed_dim, num_heads, ff_dim, num_layers, dropout = 0.1, device = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.encoder = TransformerEncoder(src_vocab_size, embed_dim, num_heads, ff_dim, num_layers, src_max_length, dropout, device)\n",
        "        self.decoder = TransformerDecoder(tgt_vocab_size, embed_dim, tgt_max_length, num_heads, ff_dim, num_layers, dropout, device)\n",
        "        self.fc = nn.Linear(in_features = embed_dim, out_features = tgt_vocab_size) \n",
        "    \n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = src.shape[1] # src shape: batch_size, src_max_length\n",
        "        tgt_mask = tgt.shape[1]\n",
        "\n",
        "        src_mask = torch.zeros((src_mask, src_mask), device = self.device).type(torch.bool)\n",
        "        tgt_mask = (torch.triu(torch.ones(\n",
        "            (tgt_mask, tgt_mask),\n",
        "            device = self.device)\n",
        "        ) == 1).transpose(0, 1)\n",
        "        tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        enc_output = self.encoder(src)\n",
        "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "OBvlk9wb1A"
      },
      "source": [
        "batch_size = 128\n",
        "src_vocab_size = 1000\n",
        "tgt_vocab_size = 2000\n",
        "embed_dim = 200\n",
        "tgt_max_length = 100\n",
        "src_max_length = 100\n",
        "num_layers = 2\n",
        "num_heads = 4\n",
        "ff_dim = 256"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "SEk85cTcc1"
      },
      "source": [
        "model = Transformer(\n",
        "    tgt_vocab_size, src_vocab_size, \n",
        "    tgt_max_length, src_max_length, \n",
        "    embed_dim, num_heads, \n",
        "    ff_dim, num_layers)\n",
        "\n",
        "src = torch . randint (\n",
        "    high = 2,\n",
        "    size = (batch_size , src_max_length),\n",
        "    dtype = torch.int64\n",
        ")\n",
        "\n",
        "tgt = torch.randint (\n",
        "    high = 2,\n",
        "    size = (batch_size, tgt_max_length),\n",
        "    dtype = torch.int64\n",
        ")\n",
        "prediction = model ( src , tgt )\n",
        "prediction.shape # batch_size x max_length x tgt_vocab_size"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "torch.Size([128, 100, 2000])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}