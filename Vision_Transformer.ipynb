{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "0jnkpjyetv"
      },
      "source": [
        "# Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "vglQG9K26M"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.datasets as ImageFolder\n",
        "import math\n",
        "import os\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "1eD6EdCnLO"
      },
      "source": [
        "#!gdown 1vSevps_hV5zhVf6aWuN8X7dd-qSAIgcc\n",
        "#!unzip ./flower_photos.zip"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "2g4LFY9Qoy"
      },
      "source": [
        "#load data\n",
        "data_path = './flower_photos'\n",
        "dataset = ImageFolder.ImageFolder(root = data_path)\n",
        "classes = dataset.classes\n",
        "num_classes = len(classes)\n",
        "num_samples = len(dataset)\n",
        "\n",
        "#split data\n",
        "VALID_RATIO, TRAIN_RATIO = 0.1, 0.8\n",
        "n_train_samples = int(num_samples * TRAIN_RATIO)\n",
        "n_val_samples = int(num_samples * VALID_RATIO)\n",
        "n_test_samples = num_samples - n_train_samples - n_val_samples\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    dataset,\n",
        "    [n_train_samples, n_val_samples, n_test_samples]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "a2puwQ6wQV"
      },
      "source": [
        "#resize and convert to tensor\n",
        "\n",
        "IMG_SIZE = 224\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "test_trainsform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "07Scgi87ZD"
      },
      "source": [
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform = test_trainsform\n",
        "test_dataset.dataset.transform = test_trainsform"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "HzEH3YtKlU"
      },
      "source": [
        "# DATALOADERS\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "9nNRaxoxCN"
      },
      "source": [
        "# TRAINING FROM SCRATCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "PlQuqoDKGq"
      },
      "source": [
        "## 4.1 Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "YOZnMA3lzv"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            batch_first = True\n",
        "        )\n",
        "        self.ffn = nn.Sequential(\n",
        "                nn.Linear(in_features = embed_dim, out_features = ff_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(in_features = ff_dim, out_features = embed_dim)\n",
        "        )\n",
        "        self.layer_norm1 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.layer_norm2 = nn.LayerNorm(normalized_shape = embed_dim, eps = 1e-6)\n",
        "        self.dropout1 = nn.Dropout(p = dropout)\n",
        "        self.dropout2 = nn.Dropout(p = dropout)\n",
        "\n",
        "    def forward(self, query, key, val):\n",
        "        attn_output, _ = self.attn(query, key, val) # self-attention\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layer_norm1(query + attn_output) # add and norm\n",
        "        ffn_output = self.ffn(out1) # feed forward\n",
        "        out2 = self.layer_norm2(out1 + ffn_output) # add and norm\n",
        "        return out2"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Ft4Jj6dzmK"
      },
      "source": [
        "class PathPositionEmbedding(nn.Module):\n",
        "    def __init__(self, img_size = 224, embed_dim = 512, patch_size = 16, device = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels = 3,\n",
        "            out_channels = embed_dim, \n",
        "            kernel_size = patch_size,\n",
        "            stride = patch_size,\n",
        "            bias = False\n",
        "        )\n",
        "        scale = embed_dim ** -0.5\n",
        "        self.position_embedding = nn.Parameter(\n",
        "            scale * torch.randn((img_size // patch_size) ** 2, embed_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)  # shape - [*, with, grid, grid]\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape - [*, with, grid * grid]\n",
        "        x = x.permute(0, 2, 1)  # shape - [*, grid * grid, with]\n",
        "        x = x + self.position_embedding\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "8DbrvGA1PH"
      },
      "source": [
        "class VisionTransformerCls(nn.Module):\n",
        "    def __init__(self, img_size, embed_dim, num_heads, ff_dim, dropout = 0.1, patch_size = 16, num_classes = 10, device = 'cpu'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.position_embedding = PathPositionEmbedding(\n",
        "            img_size = img_size,\n",
        "            embed_dim = embed_dim,\n",
        "            patch_size = patch_size,\n",
        "            device = device\n",
        "        )\n",
        "        self.transformer_layer = TransformerEncoder(\n",
        "            embed_dim = embed_dim,\n",
        "            num_heads = num_heads,\n",
        "            ff_dim = ff_dim,\n",
        "            dropout = dropout\n",
        "        )\n",
        "        #self.pooling = nn.AvgPool1d(kernel_size = max_length)\n",
        "        self.fc1 = nn.Linear(in_features = embed_dim, out_features = 20)\n",
        "        self.fc2 = nn.Linear(in_features = 20, out_features = num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ReLu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        output = self.position_embedding(x)\n",
        "        out_put = self.transformer_layer(output, output, output)\n",
        "        out_put = output[:, 0, :]\n",
        "        out_put = self.dropout(out_put)\n",
        "        out_put = self.fc1(out_put)\n",
        "        out_put = self.ReLu(out_put)\n",
        "        out_put = self.dropout(out_put)\n",
        "        out_put = self.fc2(out_put)\n",
        "        return out_put"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "JGypu2sPgC"
      },
      "source": [
        "## 4.2  Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "aKwCbt0pwZ"
      },
      "source": [
        "def train_epoch(model, optimizer, criterion, data_loader, device, epoch = 0, log_interval = 50):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    losses = []\n",
        "    times = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (images, labels) in enumerate(data_loader):\n",
        "        # dua vao device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # predict\n",
        "        predictions = model(images)\n",
        "\n",
        "        # tinh loss\n",
        "        loss = criterion(predictions, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_acc += (predictions.argmax(1) == labels).sum().item()\n",
        "        total_count += labels.size(0) # so luong anh\n",
        "\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(\"| epoch {:3d} | {:5d} / {:5d} batches |\"\n",
        "                  \"| accuracy: {:8.3f}% |\".format(epoch, idx, len(data_loader), total_acc / total_count * 100))\n",
        "            total_acc, total_count = 0,0\n",
        "            start_time = time.time()\n",
        "    epoch_acc = total_acc / total_count * 100\n",
        "    epoch_loss = sum(losses) / len(losses)\n",
        "    return epoch_acc, epoch_loss\n",
        "\n",
        "def evaluate(model, criterion, data_loader, device):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for idx, (images, labels) in enumerate(data_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            predictions = model(images)\n",
        "            loss = criterion(predictions, labels)\n",
        "            losses.append(loss.item())\n",
        "            total_acc += (predictions.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "    epoch_acc = total_acc / total_count * 100\n",
        "    epoch_loss = sum(losses) / len(losses)\n",
        "    return epoch_acc, epoch_loss\n",
        "\n",
        "def train_model(model, model_name, save_model, optimizer, criterion, train_loader, val_loader, num_epochs, device):\n",
        "    train_accs, train_losses = [], [] # su dung de visualize\n",
        "    val_accs, val_losses = [], [] # su dung de visualize\n",
        "    best_loss_acc = 100 # gia tri khoi tao dung de luu nhung model tot\n",
        "    times = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        train_acc, train_loss = train_epoch(model, optimizer, criterion, train_loader, device, epoch)\n",
        "        train_accs.append(train_acc)\n",
        "        train_losses.append(train_loss)\n",
        "        # evaluate\n",
        "\n",
        "        val_acc, val_loss = evaluate(model, criterion, val_loader, device)\n",
        "        val_accs.append(val_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        # save model\n",
        "        if val_loss < best_loss_acc:\n",
        "            torch.save(model.state_dict(), save_model + f'/{model_name}.pt')\n",
        "        times.append(time.time() - epoch_start_time)\n",
        "        print('-' * 59)\n",
        "        # Print loss and acc end of epoch\n",
        "        print(\"| End of epoch {:3d} | Time: {:5.2f}s | Train Accuracy: {:8.3f}% | Train Loss: {:8.3f} \"\n",
        "              \"| Val Accuracy: {:8.3f}% | Val Loss: {:8.3f} \".format(epoch, time.time() - epoch_start_time, train_acc, train_loss, val_acc, val_loss))\n",
        "        print('-' * 59) \n",
        "    # load best model\n",
        "    model.load_state_dict(torch.load(save_model + f'/{model_name}.pt'))\n",
        "    model.eval()\n",
        "    metrics = {\n",
        "        'train_acc': train_accs,\n",
        "        'train_loss': train_losses,\n",
        "        'val_acc': val_accs,\n",
        "        'val_loss': val_losses,\n",
        "        'time': times\n",
        "    }\n",
        "    return model, metrics\n",
        "\n",
        "def plot_result(num_epochs, train_accs, train_losses, val_accs, val_losses):\n",
        "    epochs = list(range(num_epochs))\n",
        "    fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 6))\n",
        "    axs[0].plot(epoch, train_accs, label = 'Train Accuracy')\n",
        "    axs[0].plot(epoch, val_accs, label = 'Val Accuracy')\n",
        "    axs[1].plot(epoch, train_losses, label = 'Train Loss')\n",
        "    axs[1].plot(epoch, val_losses, label = 'Val Loss')\n",
        "    axs[0].set_title('Accuracy')\n",
        "    axs[1].set_title('Loss')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    plt.legend()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "tBBzFgiLb6"
      },
      "source": [
        "img_size = 224\n",
        "embed_dim = 512\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "dropout = 0.1\n",
        "num_classes = len(classes)\n",
        "path_size = 16\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "5"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "mYP5R7KF8f"
      },
      "source": [
        "model = VisionTransformerCls(\n",
        "    img_size = 224, embed_dim = embed_dim, num_heads = num_heads, ff_dim = ff_dim, num_classes = num_classes, device = device\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
        "\n",
        "num_epochs = 50\n",
        "save_model = './vit_flower'\n",
        "\n",
        "os.makedirs(save_model, exist_ok = True)\n",
        "model_name = 'vit_flower'\n",
        "\n",
        "model, metric = train_model(\n",
        "    model, model_name, save_model, optimizer, criterion, train_loader, val_loader, num_epochs, device\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "cBn4WLVyHK"
      },
      "source": [
        "# 5.0 Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "BPyKbGy1dy"
      },
      "source": [
        "from transformers import ViTForImageClassification\n",
        "id2label = {idx : label for idx, label in enumerate(classes)}\n",
        "label2id = {label : idx for idx, label in enumerate(classes)}\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', \n",
        "                                                  num_labels = num_classes,\n",
        "                                                  id2label = id2label,\n",
        "                                                  label2id = label2id\n",
        "                                                  )\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "m0ym3KZN3b"
      },
      "source": [
        "!pip install -q datasets accelerate evaluate"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "6LKY9IJrfw"
      },
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load('accuracy')\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis = 1)\n",
        "    return metric.compute(predictions = predictions, references = labels)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Lf36jMuUeW"
      },
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "VG0YEWtRBK"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "metric_name = \"accuracy\"\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"vit_flowers\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    logging_dir='logs',\n",
        "    remove_unused_columns=False,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "LTxTABBzvv"
      },
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(examples):\n",
        "    # example => Tuple(image, label)\n",
        "    pixel_values = torch.stack([example[0] for example in examples])\n",
        "    labels = torch.tensor([example[1] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=feature_extractor,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "h4Dh8nE5px"
      },
      "source": [
        "import wandb\n",
        "wandb.init(mode='disabled')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "<wandb.sdk.wandb_run.Run at 0x7ddfdd39b050>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "bJyPWACYwX"
      },
      "source": [
        "trainer.train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)\nCell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\nFile \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2165\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2166\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2167\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2169\u001b[0m     )\n\nFile \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:2522\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2516\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2517\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2520\u001b[0m )\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2528\u001b[0m ):\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\nFile \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:3688\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3686\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3689\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\nFile \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/accelerate/accelerator.py:2454\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2454\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\nFile \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    628\u001b[0m )\n\nFile \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n\nFile \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n\n\u001b[0;31mKeyboardInterrupt\u001b[0m: \n"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "sIXeuDe4gs"
      },
      "source": [
        "outputs = trainer.predict(test_dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "gX6JLpvhCi"
      },
      "source": [
        "outputs.metrics"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}